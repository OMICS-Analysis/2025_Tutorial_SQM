---
title: "Análisis de Metagenoma usando SqueezeMeta"
author: "OMICs Analysis"
format: 
  html:
    toc: true
    toc-depth: 3
    theme: journal
    toc-location: left-body
    smooth-scroll: true
    toc-title: "Contenido"
editor: visual
execute:
  engine: knitr
lang: es
---

![](images/logo_OAN_for_white.png){fig-align="center" width="300"}

# Introducción

ADVERTENCIA ⚠️: En este tutorial vamos a echar a andar el flujo de trabajo [`SqueezeMeta`](https://github.com/jtamames/SqueezeMeta), ten en cuenta que para este punto debes tener suficiente espacio de memoria (`470Gb`) en tu computadora para generar la base de datos que utiliza esta herramienta. También ten en cuenta tener mínimo `8Gb` de `RAM`.

Los siguiente análisis se realizaron en un equipo `Gigabyte Technology Co., Ltd. H67MA-USB3-B3` procesador `Intel® Core™ i7-2600 CPU @ 3.40GHz × 8` con `32Gb` de `RAM`. Con un sistema operativo `Ubuntu 22.04.5 LTS`.

Para este tutorial vamos a trabajar con datos del estudio "[Effects of heat stress on 16S rDNA, metagenome and metabolome in Holstein cows at different growth stages](https://www.nature.com/articles/s41597-022-01777-6)". Para fines educativos solo usaremos un subconjunto de los datos de las vacas Holstein en tres etapas de crecimiento, en condiciones normales y con estrés térmico.

| SRA ID | Condición | Etapa de crecimiento |
|------------------------|------------------------|------------------------|
| [SRR19746212](https://trace.ncbi.nlm.nih.gov/Traces/sra?run=SRR19746212) | Periodo Normal | Novillas en crecimiento |
| [SRR19746215](https://trace.ncbi.nlm.nih.gov/Traces/sra?run=SRR19746215) | Estrés Térmico | Novillas en crecimiento |
| [SRR19746202](https://trace.ncbi.nlm.nih.gov/Traces/sra?run=SRR19746202) | Periodo Normal | Novillas |
| [SRR19746205](https://trace.ncbi.nlm.nih.gov/Traces/sra?run=SRR19746205) | Estrés Térmico | Novillas |
| [SRR19746208](https://trace.ncbi.nlm.nih.gov/Traces/sra?run=SRR19746208) | Periodo Normal | Vacas Lactantes |
| [SRR19746219](https://trace.ncbi.nlm.nih.gov/Traces/sra?run=SRR19746219) | Estrés Térmico | Vacas Lactantes |

## Preparación de datos

Para descargar los datos usaremos [`RSA Toolkit`](https://github.com/ncbi/sra-tools/wiki/02.-Installing-SRA-Toolkit) y [`parallel-fastq-dump`](https://github.com/rvalieris/parallel-fastq-dump), a continuación el ejemplo para uno. Por otra parte vamos a generar nuestro directorio de trabajo.

```{bash, eval=FALSE}
mkdir 2025_Demo_sqm
cd 2025_Demo_sqm
mkdir data
cd data
```

Este mismo proceso se realizó para todos los `SRA ID`.

```{bash, eval=FALSE}
prefetch SRR19746212

parallel-fastq-dump \
--sra-id SRR19746212 \
-t 7 \
--outdir ./ \
--split-files \
--gzip
```

## Pre procesamiento y control de calidad

[`Trimmomatic`](https://github.com/timflutre/trimmomatic) es una herramienta flexible y eficiente diseñada específicamente para el preprocesamiento de datos de secuenciación de próxima generación (NGS) generados por plataformas Illumina. Se utiliza principalmente para realizar tareas de recorte (trimming) y filtrado de lecturas en formato FASTQ, tanto en datos paired-end como single-end. Sus funciones clave incluyen:

-   **Remoción de adaptadores**: Elimina secuencias de adaptadores Illumina que pueden contaminar las lecturas, lo que es esencial para evitar problemas en alineamientos posteriores.

-   **Recorte de bases de baja calidad**: Corta bases al inicio o final de las lecturas que no cumplen con un umbral de calidad (por ejemplo, usando Phred scores), mejorando la calidad general de los datos.

-   **Filtrado de lecturas**: Descarta lecturas cortas, de baja calidad o con patrones no deseados, como secuencias de baja complejidad.

-   **Otras operaciones**: Puede realizar cropping (recorte fijo de longitud) y sliding window para evaluar calidad en ventanas móviles.

Esto es particularmente útil en flujos de trabajo de metagenómica, transcriptómica o genómica con datos Illumina, ya que ayuda a limpiar los datos crudos antes de análisis downstream como ensamblaje o mapeo, reduciendo errores y mejorando la precisión.

[`Cutadapt`](https://cutadapt.readthedocs.io/en/stable/installation.html), por su parte, es una herramienta versátil enfocada en la remoción de adaptadores y secuencias no deseadas de lecturas de secuenciación de alto rendimiento, con un énfasis en datos Illumina. Sus usos principales en secuencias Illumina son:

-   **Remoción de adaptadores**: Busca y elimina adaptadores (como los universales de Illumina) en cualquier posición de las lecturas, ya sea al 3' o 5', o incluso en el medio, lo que es común en datos paired-end o single-end contaminados.

-   **Recorte basado en calidad**: Puede trimmed bases de baja calidad desde los extremos de las lecturas, similar a Trimmomatic, pero con opciones más flexibles para filtrado.

-   **Filtrado y modificación de lecturas**: Permite descartar lecturas demasiado cortas, con mismatches excesivos o que no cumplan criterios específicos, y soporta operaciones como demultiplexing (separación por barcodes).

-   **Otras funcionalidades**: Maneja poly-A tails, secuencias repetitivas o primers, y es eficiente para grandes volúmenes de datos.

### Cutadapt

Cutadapt es util para limpiar lecturas paired-end de Illumina en archivos FASTQ:

-   **`-a`** y **`-A`**: Elimina adaptadores específicos de Illumina (forward: GATCGGAAGAGCACACGTCTGAACTCCAGTCA; reverse: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT).

-   **`--trim-n`**: Recorta bases "N" (indeterminadas) de los extremos.

-   **`--match-read-wildcards`**: Permite comodines en las lecturas para coincidencias flexibles.

```{bash, eval=FALSE}
conda activate cutadapt

mkdir 1_QC
cd 2025_Demo_sqm
```

```{bash, eval=FALSE}
cutadapt \
-a GATCGGAAGAGCACACGTCTGAACTCCAGTCA \
-A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT \
--trim-n \
--match-read-wildcards -j 7 \
-o "1_QC/SRR19746202_1.fastq.gz" \
-p "1_QC/SRR19746202_2.fastq.gz" \
data/SRR19746202_1.fastq.gz \
data/SRR19746202_2.fastq.gz > SRR19746202_cutadapt.log 2>&1

```

### Trimmomatic

El siguiente código lo que hace aplicar filtros:

-   `LEADING:15`: Elimina bases iniciales con calidad `<15`.

-   `TRAILING:15`: Elimina bases finales con calidad `<15`.

-   `SLIDINGWINDOW:10:15`: Recorta si el promedio de calidad en ventana de 10 bases es `<15`.

-   `AVGQUAL:25`: Descarta lecturas con calidad promedio `<25`.

-   `MINLEN:50`: Descarta lecturas `<50` bases.

```{bash, eval=FALSE}
java -jar bin/trimmomatic.jar PE \
-threads 7 \
-phred33 \
"1_QC/SRR19746202_1.fastq.gz" "1_QC/SRR19746202_2.fastq.gz"\
"1_QC/SRR19746202_1_trimmed.fastq.gz" "SRR19746202_1_discard.fastq" \
"1_QC/SRR19746202_2_trimmed.fastq.gz" "SRR19746202_2_discard.fastq" \
LEADING:15 TRAILING:15 SLIDINGWINDOW:10:15 AVGQUAL:25 MINLEN:50 > "SRR19746202_trimmomatic.log" 2>&1
```

## Quitar contaminación del Hospedero

```{bash, eval=FALSE}
sudo apt install bowtie2
```

### Creación de base de datos del hospedero (*Bos taurus*)

Descarga de su genoma

```{bash, eval=FALSE}
cd data

wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/002/263/795/GCF_002263795.3_ARS-UCD2.0/GCF_002263795.3_ARS-UCD2.0_genomic.fna.gz
```

```{bash, eval=FALSE}
mv GCF_002263795.3_ARS-UCD2.0_genomic.fna.gz bos_taurus_host.fna.gz
mkdir db_host
```

Indexación para creación de base de datos

```{bash, eval=FALSE}
bowtie2-build --seed 123 bos_taurus_host.fna.gz db_host
```

### Alineamiento contra hospedero

Alineamiento de fastq contra la base de datos

```{bash, eval=FALSE}
bowtie2 \
--very-sensitive-local \
-p 7 --seed 123 \
-x ./data/db_host/db_host \
-1 data/SRR19746202_1.fastq.gz \
-2 data/SRR19746202_2.fastq.gz \
-S 1_QC/host_filter/SRR19746202.sam
```

⚠️El archivo `.sam` es muy pesado, hasta `20Gb`, es indispensable comprimirlo y eliminarlo si deseas.

```{bash, eval=FALSE}
samtools view \
-@ 7 \
-b -f 12 -F 256 \
1_QC/host_filter/SRR19746202.sam > 1_QC/host_filter/SRR19746202.bam
```

```{bash, eval=FALSE}
samtools sort \
-n 1_QC/host_filter/SRR19746202.bam \
-o 1_QC/host_filter/SRR19746202.sorted.bam
```

### Creación de archivo FASTQ *interleaved* (pareado)

```{bash, eval=FALSE}
samtools bam2fq \
1_QC/host_filter/SRR19746202.sorted.bam > 1_QC/host_filter/SRR19746202.fastq
```

### Automatización

Para hacerlo con todas las muestras automatizamos el proceso, y de paso comprimir el archivo `.fastq`, creamos un script llamado `host_filter.sh`:

```{bash, eval=FALSE}
#| code-fold: true
#| code-summary: "host_filter.sh"

#!/usr/bin/env bash
set -euo pipefail

# host_filter.sh
# Pipeline simple: bowtie2 -> samtools (filtrar pares no mapeados) -> convertir a FASTQ comprimido (.fastq.gz)
# Uso: ./host_filter.sh                # usa la lista por defecto
#       ./host_filter.sh 19746205 19746208   # procesa muestras dadas (acepta con o sin prefijo SRR)

samples=(19746202 19746205 19746208 19746212 19746215 19746219)
if [ "$#" -gt 0 ]; then
  samples=("$@")
fi

INDEX="./data/db_host/db_host"
OUTDIR="1_QC/host_filter"
THREADS=7
SEED=123

mkdir -p "$OUTDIR"

command -v bowtie2 >/dev/null 2>&1 || { echo "ERROR: bowtie2 no está en PATH" >&2; exit 1; }
command -v samtools >/dev/null 2>&1 || { echo "ERROR: samtools no está en PATH" >&2; exit 1; }

for s in "${samples[@]}"; do
  SAMPLE="$s"
  if [[ "$SAMPLE" != SRR* ]]; then
    SAMPLE="SRR${SAMPLE}"
  fi

  R1="data/${SAMPLE}_1.fastq.gz"
  R2="data/${SAMPLE}_2.fastq.gz"
  SAM="$OUTDIR/${SAMPLE}.sam"
  LOG="$OUTDIR/${SAMPLE}.log"
  BAM="$OUTDIR/${SAMPLE}.bam"
  SORTED="$OUTDIR/${SAMPLE}.sorted.bam"
  FQ_GZ="$OUTDIR/${SAMPLE}.fastq.gz"

  echo "=== Procesando ${SAMPLE} ===" | tee -a "$LOG"

  if [ ! -f "$R1" ]; then
    echo "ERROR: $R1 no existe. Saltando ${SAMPLE}." | tee -a "$LOG"
    continue
  fi
  if [ ! -f "$R2" ]; then
    echo "ERROR: $R2 no existe. Saltando ${SAMPLE}." | tee -a "$LOG"
    continue
  fi

  # 1) Bowtie2 -> SAM (stdout+stderr al log)
  echo "[1] bowtie2 -> SAM" | tee -a "$LOG"
  bowtie2 --very-sensitive-local -p "$THREADS" --seed "$SEED" \
    -x "$INDEX" -1 "$R1" -2 "$R2" -S "$SAM" >>"$LOG" 2>&1

  # 2) Filtrar lecturas donde ambos pares NO se alinearon al host (-f 12) y convertir a BAM
  echo "[2] samtools view -b -f 12 -F 256 -> BAM" | tee -a "$LOG"
  samtools view -@ "$THREADS" -b -f 12 -F 256 "$SAM" -o "$BAM" 2>>"$LOG"

  # eliminar SAM para ahorrar espacio
  rm -f "$SAM"

  # 3) Ordenar por nombre
  echo "[3] samtools sort -n -> $SORTED" | tee -a "$LOG"
  samtools sort -n -@ "$THREADS" -o "$SORTED" "$BAM" 2>>"$LOG"
  rm -f "$BAM"

  # 4) Convertir a FASTQ comprimido (.fastq.gz)
  echo "[4] samtools bam2fq -> gzip -> $FQ_GZ" | tee -a "$LOG"
  samtools bam2fq "$SORTED" | gzip -c > "$FQ_GZ" 2>>"$LOG"

  # limpiar intermedios
  rm -f "$SORTED"

  echo "=== Hecho ${SAMPLE}. Salida: $FQ_GZ . Log: $LOG ===" | tee -a "$LOG"
  echo
done

echo "Pipeline finalizado. Archivos en: $OUTDIR"
```

```{bash, eval=FALSE}
chmod +x host_filter.sh

./host_filter
```

⚠️**PEROOO**, necesitamos las lecturas pareada para `SqueezeMeta`, es por ello que utilizamos el siguiente código de `Python` para separarlas. A este archivo le llamamos `sort_fastq.py`:

```{python, eval=FALSE}
#| code-fold: true
#| code-summary: "sort_fastq.py:"

#!/usr/bin/env python3
import gzip
import sys

if len(sys.argv) != 4:
    print("Uso: split_interleaved_fastq.py IN.fastq.gz OUT_1.fastq.gz OUT_2.fastq.gz", file=sys.stderr)
    sys.exit(1)

inp, out1, out2 = sys.argv[1], sys.argv[2], sys.argv[3]

with gzip.open(inp, "rt") as inf, gzip.open(out1, "wt") as o1, gzip.open(out2, "wt") as o2:
    buf = []
    for i, line in enumerate(inf, 1):
        buf.append(line)
        if i % 8 == 0:
            # primer par (4 líneas) -> out1; segundo par (4 líneas) -> out2
            o1.writelines(buf[:4])
            o2.writelines(buf[4:])
            buf = []
    # por si queda algo al final (archivo truncado o similar)
    if buf:
        if len(buf) <= 4:
            o1.writelines(buf)
        else:
            o1.writelines(buf[:4])
            o2.writelines(buf[4:])
```

```{bash, eval=FALSE}
chmod +x sort_fastq.py

python3 sort_fastq.py \
1_QC/host_filter/SRR19746202.fastq.gz \
1_QC/host_filter/SRR19746202_1.fastq.gz \
1_QC/host_filter/SRR19746202_2.fastq.gz

```

✅**Extra**. Con este código puedes ver en cada paso la cantidad de lecturas que hay en cada archivo FASTQ:

```{bash, eval=FALSE}
echo "R1 reads: $(( $(zcat 1_QC/host_filter/SRR19746202_1.fastq.gz | wc -l) / 4 ))" 
echo "R2 reads: $(( $(zcat 1_QC/host_filter/SRR19746202_2.fastq.gz | wc -l) / 4 ))"

```

# SqueezeMeta

Para iniciar con el flujo de trabajo de SqueezeMeta necesitagenerar un archivo de table-separate-value con el nombre de los archivos, lo nombraremos `samples.samples`

```{bash, eval=FALSE}
#| code-fold: true
#| code-summary: "samples.samples:"
SRR19746202	SRR19746202_1.fastq.gz	pair1
SRR19746202	SRR19746202_2.fastq.gz	pair2
SRR19746205	SRR19746205_1.fastq.gz	pair1
SRR19746205	SRR19746205_2.fastq.gz	pair2
SRR19746208	SRR19746208_1.fastq.gz	pair1
SRR19746208	SRR19746208_2.fastq.gz	pair2
SRR19746212	SRR19746212_1.fastq.gz	pair1
SRR19746212	SRR19746212_2.fastq.gz	pair2
SRR19746215	SRR19746215_1.fastq.gz	pair1
SRR19746215	SRR19746215_2.fastq.gz	pair2
SRR19746219	SRR19746219_1.fastq.gz	pair1
SRR19746219	SRR19746219_2.fastq.gz	pair2
```

```{bash, eval=FALSE}
conda activate SqueezeMeta

SqueezeMeta.pl \
-m sequential \
-s 1_QC/host_filter/samples.samples \
-f 1_QC/host_filter \
-t 7 \
--lowmem

conda deactivate
```

------------------------------------------------------------------------

### Archivos principales que genera SqueezeMeta (organizados por etapa)

> **Estructura general**:
>
> ```         
> SRR19746202/
> ├─ results/
> │  ├─ 01.SRR19746202.fasta
> │  ├─ 02.SRR19746202.rnas
> │  ├─ 03.SRR19746202.fna
> │  ├─ 03.SRR19746202.faa
> │  ├─ 06.SRR19746202.fun3.tax.wranks
> │  ├─ 07.SRR19746202.fun3.kegg
> │  ├─ 10.SRR19746202.mappingstat
> │  ├─ results/bins/...
> │  └─ tables/  ← tablas agregadas (phylum.abund.tsv, kegg.tpm.tsv, etc.)
> └─ intermediate/
>    └─ (muchos archivos intermedios: diamond, pfam, contigcov, mapcount...)
> ```

------------------------------------------------------------------------

### Ensamblaje

-   **`results/01.SRR19746202.fasta`** Contiene los contigs resultantes del ensamblaje (fasta). **Uso:** inspección de contigs, generación de ORFs, entrada para binning (anvì'o, metaWRAP, etc.), extracción de secuencias para búsquedas BLAST o para construir pangenomas.

-   **`intermediate/01.SRR19746202.lon`** y **`intermediate/01.SRR19746202.stats`** Longitudes y estadísticas del ensamblaje (N50, N90, nº de contigs, etc.). **Uso:** control de calidad del ensamblaje y comparaciones entre muestras.

------------------------------------------------------------------------

### Detección de RNAs

-   **`results/02.SRR19746202.rnas`** (fasta con rRNAs y tRNAs encontrados) **Uso:** localizar 16S/23S para taxonomía basada en 16S, filtrar rRNA de contigs, o extraer genes ribosomales para análisis filogenéticos.

-   **`results/02.SRR19746202.16S`** Asignaciones taxonómicas (ej. RDP) de las 16S detectadas. **Uso:** obtener una vista rápida de taxa identificables por 16S.

------------------------------------------------------------------------

### Predicción de genes (ORFs)

-   **`results/03.SRR19746202.fna`** Secuencias nucleotídicas de ORFs predichos (CDS). **Uso:** anotación funcional y cálculo de abundancias por gen.

-   **`results/03.SRR19746202.faa`** Secuencias aminoacídicas de los ORFs predichos. **Uso:** búsquedas HMM (pfam), BLAST/DIAMOND contra bases de datos funcionales/taxonómicas, clustering, construcción de pangenoma de genes.

-   **`results/03.SRR19746202.gff`** Posiciones de genes en los contigs (GFF). **Uso:** visualizar en genome browser, extraer ORFs por coordenadas, integrar con anotaciones externas.

------------------------------------------------------------------------

### Anotaciones homólogas y HMMs

-   **`intermediate/04.SRR19746202.nr.diamond`**, **`04.SRR19746202.kegg.diamond`**, **`04.SRR19746202.eggnog.diamond`** Resultados DIAMOND (hits) contra bases NR/KEGG/eggNOG. **Uso:** base para asignación taxonómica y funcional de ORFs (mejores hits, IDs KEGG/COG).

-   **`intermediate/05.SRR19746202.pfam.hmm`** Resultados HMM (pfam) — dominios detectados. **Uso:** anotación de dominios, categorización funcional, agrupamiento por familias.

------------------------------------------------------------------------

### Asignación taxonómica y funcional

-   **`results/06.SRR19746202.fun3.tax.wranks`** (y la variante `noidfilter`) Taxonomía asignada por ORF con rangos taxonómicos (kingdom→species). **Uso:** construir perfiles taxonómicos por gen/contig/bin; filtrar por identidad; estimar composición (abundancias por taxón).

-   **`results/07.SRR19746202.fun3.cog`**, **`results/07.SRR19746202.fun3.kegg`**, **`results/07.SRR19746202.pfam`** Asignaciones funcionales (COG, KEGG, PFAM) por ORF. **Uso:** agrupar funciones, análisis funcional (pathways, enriquecimiento), generación de tablas de abundancia funcional.

------------------------------------------------------------------------

### Mapeo de lecturas y medidas de abundancia

-   **`results/10.SRR19746202.mappingstat`** Estadísticas globales de mapeo (por muestra). **Uso:** control de calidad del mapeo, porcentaje de lecturas mapeadas.

-   **`intermediate/10.SRR19746202.mapcount`** Tabla con: ORF, longitud, nº reads mapeados, bases, RPKM, cobertura, TPM, y muestra. **Uso:** calcular abundancias por gen, normalizar y comparar entre muestras; entrada para `sqm2tables.py`.

-   **`intermediate/10.SRR19746202.contigcov`** Cobertura/RPKM/TPM por contig en cada muestra. **Uso:** detectar contigs altamente cubiertos, evaluar robustez de bins.

------------------------------------------------------------------------

### Binning (MAGs)

-   **`intermediate/binners/metabat/*`**, **`intermediate/binners/maxbin/*`**, **`results/bins/`** Carpeta con archivos FASTA por bin generados por los distintos binners y, finalmente, los bins integrados por DAS Tool (en `results/bins/`). **Uso:** obtener MAGs para análisis taxonómico/funzional, evaluar calidad (completeness/contamination).

-   **`intermediate/17.SRR19746202.checkM`** y opcionalmente **`intermediate/17.SRR19746202.gtdbtk`** Resultados de CheckM2 (completeness/contamination) y GTDB-Tk (clasificación GTDB si se solicitó). **Uso:** filtrar MAGs de alta calidad, nombrado taxonómico robusto.

-   **`results/18.SRR19746202.bintable`** Tabla resumen con metadatos de cada bin: nombre, taxonomía, tamaño, GC, nº contigs, completeness, contamination, y abundancias (coverage/TPM) por muestra. **Uso:** selección de bins para downstream (pangenoma, phylogenomics, ensamblado de genomas).

------------------------------------------------------------------------

### Tablas agregadas (script `sqm2tables.py`)

Al final SqueezeMeta ejecuta (o puedes ejecutar) `sqm2tables.py` que genera tablas con features (taxones/funciones) en filas y muestras en columnas. Estos ficheros van a `SRR19746202/results/tables/`. Algunos archivos clave:

> Ejemplos de archivos en `results/tables/` y su utilidad.

-   **`SRR19746202.orfs.sequences.tsv`** Secuencias de ORFs listadas en formato tabular (útil para extraer secuencias concretas).

-   **`SRR19746202.orf.tax.*.tsv`** (varias variantes: `allfilter`, `prokfilter`, `nofilter`) Taxonomía por ORF con distintos criterios de filtrado de identidad. **Uso:** construir perfiles taxonómicos agregados por contig/bin o por rank.

-   **`SRR19746202.<RANK>.allfilter.abund.tsv`** (ej. `SRR19746202.phylum.allfilter.abund.tsv`) Tabla con abundancia (número de reads) por taxón y muestra para cada nivel taxonómico (superkingdom, phylum, class, order, family, genus, species). **Uso:** análisis de composición taxonómica y cálculo de diversidad (α y β).

-   **`SRR19746202.KO.abunds.tsv`, `SRR19746202.COG.abunds.tsv`, `SRR19746202.PFAM.abunds.tsv`** Abundancias por función (KO/COG/PFAM). **Uso:** análisis funcional y de pathways; normalización a TPM (`*.tpm.tsv`) y cálculo de copy-numbers (`*.copyNumber.tsv`).

-   **`SRR19746202.<classification>.tpm.tsv`** Tablas normalizadas (TPM) para comparaciones entre muestras.

-   **Archivos derivados:** `SRR19746202.<rank>.prokfilter.abund.tsv`, `SRR19746202.<classification>.names.tsv`, `SRR19746202.RecA.tsv` (usada para normalizar a copias/genoma). **Uso:** todo lo anterior (copias, normalizaciones, filtros).

------------------------------------------------------------------------

### ¿Qué puedes hacer con cada tipo de archivo? — ejemplos prácticos

-   **`01.*.fasta` (contigs)** → re-ensamblado, extracción de secuencias, análisis genómico, alimentación para binners/anvì'o.
-   **`03.*.faa` / `04.*.diamond` / `05.*.pfam.hmm`** → armar matrices funcionales (KEGG/COG/PFAM) y detectar funciones relevantes (resistencia a antibióticos, rutas metabólicas, dominios).
-   **`10.*.mapcount` / `contigcov`** → calcular TPM/RPKM/coverage por gen/contig y comparar abundancias entre muestras (o en una sola muestra, identificar genes más expresados/abundantes).
-   **`13.*.orftable`** → tabla maestra con anotaciones combinadas (taxonomía + función + métricas de abundancia) — ideal para filtros y para construir queries del estilo “toma todos los ORFs asignados a COG X y con TPM \> Y”.
-   **`results/tables/*.abund.tsv`** → entrada directa a R/phyloseq/SQMtools para calcular índices de diversidad, hacer heatmaps, PCoA, clustering, etc.

------------------------------------------------------------------------

### Código en **R**: calcular y graficar la diversidad (α) de la muestra `SRR19746202`

Este ejemplo asume que ejecutaste SqueezeMeta y que `SRR19746202/results/tables/SRR19746202.phylum.allfilter.abund.tsv` existe. El script:

-   Lee la tabla de abundancias a nivel **phylum** (puedes cambiar a `genus` o `species`).
-   Calcula índices de diversidad α: **Riqueza observada**, **Índice de Shannon** y **Índice de Simpson** (usando `vegan`).
-   Grafica un barplot de las 10 phyla más abundantes y muestra los índices en el título/subtítulo.

``` r
# R: gráfica de diversidad α para SRR19746202 (SqueezeMeta)
# Requisitos: install.packages(c("readr","dplyr","tibble","ggplot2","vegan"))
library(readr)
library(dplyr)
library(tibble)
library(ggplot2)
library(vegan)

# --- Ajusta estas variables según tu proyecto ---
project <- "SRR19746202"                         # nombre del proyecto SqueezeMeta
tables_dir <- file.path(project, "results", "tables")  # ruta a tables
tax_rank_file <- file.path(tables_dir, paste0(project, ".phylum.allfilter.abund.tsv"))
sample_name <- "SRR19746202"                    # nombre de la muestra/columna esperada

# --- Carga la tabla (la primera columna suele ser el nombre del taxón) ---
if(!file.exists(tax_rank_file)){
  stop("No encontré el archivo: ", tax_rank_file, "\nAsegúrate que 'sqm2tables.py' se ejecutó y la ruta está correcta.")
}

taxa_tbl <- read_tsv(tax_rank_file, col_types = cols(.default = "c"))  # leer todo como texto al inicio

# La tabla típica tiene: TAXON <TAB> Sample1 <TAB> Sample2 ...
# Convertir valores de abundancia a numérico (saltar el nombre del taxón)
taxa_tbl <- taxa_tbl %>%
  rename(Taxon = 1) %>%
  mutate(across(-Taxon, ~ as.numeric(.)))

# Verifica que la muestra exista
if(!sample_name %in% colnames(taxa_tbl)){
  stop("La muestra '", sample_name, "' no está en las columnas del archivo. Columnas disponibles:\n",
       paste(colnames(taxa_tbl), collapse=", "))
}

# Extrae conteos para la muestra
counts <- taxa_tbl %>%
  select(Taxon, all_of(sample_name)) %>%
  rename(Count = all_of(sample_name)) %>%
  mutate(Count = ifelse(is.na(Count), 0, Count))

# Filtrar phyla con 0 conteos (opcional)
counts_nonzero <- counts %>% filter(Count > 0)

# --- Cálculo de índices de diversidad (vegan expects a numeric vector o matriz) ---
# Richness (observed), Shannon, Simpson
abund_vector <- counts_nonzero$Count
richness_obs <- sum(abund_vector > 0)
shannon_H <- diversity(abund_vector, index = "shannon")     # ln base
simpson_D <- diversity(abund_vector, index = "simpson")     # 1 - sum(p_i^2) (depending on vegan's impl.)

# Convertimos Simpson a "1 - D" como índice de diversidad de Simpson (si quieres la "simpson reciprocal" usa 1/D)
simpson_index <- 1 - simpson_D

# Resumen de índices
diversity_summary <- tibble(
  Sample = sample_name,
  Observed_richness = richness_obs,
  Shannon_H = round(shannon_H, 4),
  Simpson = round(simpson_index, 4)
)
print(diversity_summary)

# --- Preparar datos para el barplot de composición (top N phyla) ---
topn <- 10
top_phyla <- counts %>%
  arrange(desc(Count)) %>%
  slice_head(n = topn) %>%
  mutate(Proportion = Count / sum(counts$Count))

# Barplot con ggplot2
p <- ggplot(top_phyla, aes(x = reorder(Taxon, Count), y = Count)) +
  geom_col() +
  coord_flip() +
  labs(
    title = paste0("Composición por phylum — muestra: ", sample_name),
    subtitle = paste0("Top ", topn, " phyla. Shannon H = ", round(shannon_H,3),
                      " | Simpson = ", round(simpson_index,3)),
    x = "Phylum",
    y = "Reads (abundancia cruda)"
  ) +
  theme_minimal(base_size = 14)

print(p)

# --- Opcional: guardar la figura ---
# ggsave(filename = paste0(project, "_", sample_name, "_phylum_top", topn, ".png"), plot = p, width = 8, height = 6)
```

#### Notas sobre el script

-   Si prefieres trabajar con **TPM** (normalizado) usa el archivo `SRR19746202.phylum.tpm.tsv` o `SRR19746202.phylum.<variant>.tpm.tsv` si los generaste en `sqm2tables`. La lógica del script es la misma, solo cambia la lectura del archivo.
-   `vegan::diversity()` calcula Shannon (H) y Simpson (por defecto `simpson` devuelve la suma $∑ p_i^2$ — aquí lo transformamos a `1 - D` para interpretarlo como probabilidad de que dos individuos sean de distinto taxón). Ajusta a la métrica que prefieras.
-   Para análisis comparativos entre **varias** muestras conviene crear una matriz (taxa × samples) y calcular rarefaction curves, índices α para cada muestra y β-diversity (Bray-Curtis, PCoA, NMDS) con `vegan` o exportar a `phyloseq`. También puedes cargar el proyecto directamente con el paquete **SQMtools** (`loadSQM()`) para aprovechar funciones pre-hechas.

------------------------------------------------------------------------

### Flujo recomendado para análisis posteriores

1.  Generar tablas agregadas con `sqm2tables.py` (si no se crearon).
2.  Cargar las tablas en R (o usar `loadSQM()` de **SQMtools**) para generar objetos `phyloseq` / `microeco` si vas a hacer ecología microbiana compleja.
3.  Para pangenome (si tu interés es comparar genomas o MAGs) extraer secuencias de `results/bins/` y usar herramientas especializadas (ppanggolin, Roary, Panaroo, etc.). (SqueezeMeta no genera directamente una "matriz pangenoma" tipo Roary; su fortaleza es la anotación, mapeo y bins).
